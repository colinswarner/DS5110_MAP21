{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools, os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sfs/qumulo/qhome/dbn5eu/ds5110/DS5110_MAP21'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list inputs\n",
    "crash_file = \"inputs/EPDO Rate.csv\" \n",
    "incident_file = \"inputs/LIIR.csv\"\n",
    "ssp_file = \"inputs/SSP schedules.csv\"\n",
    "volume_file = \"inputs/Vol-VoverC all.csv\"\n",
    "lane_file = \"inputs/num lanes.csv\"\n",
    "terrain_file = \"inputs/terrain.csv\"\n",
    "truckpct_file = \"inputs/Truck Pct.csv\"\n",
    "areatype_file = \"inputs/urban_rural.csv\"\n",
    "LOTTR_file = \"inputs/rel_unrel.csv\"\n",
    "TMCattribute_file = \"inputs/miles urbanCode.csv\"\n",
    "county_file = \"inputs/county_district.csv\"\n",
    "TMC_file = \"inputs/TMC metadata.csv\"\n",
    "dir_AADT_file = \"inputs/Dir AADT.csv\"\n",
    "num_days_file = \"inputs/num days in data year.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# read inputs\n",
    "crash_data = spark.read.csv(crash_file, header=True)\n",
    "incident_data = spark.read.csv(incident_file, header=True)\n",
    "ssp_data = spark.read.csv(ssp_file, header=True)\n",
    "volume_data = spark.read.csv(volume_file, header=True)\n",
    "lane_data = spark.read.csv(lane_file, header=True)\n",
    "terrain_data = spark.read.csv(terrain_file, header=True)\n",
    "truckpct_data = spark.read.csv(truckpct_file, header=True)\n",
    "areatype_data = spark.read.csv(areatype_file, header=True)\n",
    "LOTTR_data = spark.read.csv(LOTTR_file, header=True)\n",
    "TMCattribute_data = spark.read.csv(TMCattribute_file, header=True)\n",
    "county_data = spark.read.csv(county_file, header=True)\n",
    "TMC_data = spark.read.csv(TMC_file, header=True)\n",
    "dir_AADT_data = spark.read.csv(dir_AADT_file, header=True)\n",
    "num_days_data = spark.read.csv(num_days_file, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# merge inputs\n",
    "sc.sql(\"set spark.sql.caseSensitive=true\")\n",
    "\n",
    "all_data = TMC_data.join(LOTTR_data, TMC_data.tmc == LOTTR_data.tmc_code, 'left_outer').drop('tmc_code')\n",
    "all_data = all_data.join(crash_data.withColumnRenamed(\"Tmc\",\"tmc\"), on=['tmc','year'], how='left_outer')\n",
    "all_data = all_data.join(incident_data.withColumnRenamed(\"Tmc\",\"tmc\"), on=['tmc','year'], how='left_outer')\n",
    "all_data = all_data.join(ssp_data.withColumnRenamed(\"Tmc\",\"tmc\").withColumnRenamed(\"Year\",\"year\"), on=['tmc','year'], how='left_outer')\n",
    "all_data = all_data.join(volume_data, on=['tmc','year'], how='left_outer')\n",
    "all_data = all_data.join(lane_data, on=['tmc','year'], how='left_outer')\n",
    "all_data = all_data.join(terrain_data, on=['tmc'], how='left_outer')\n",
    "all_data = all_data.join(truckpct_data, on=['tmc'], how='left_outer')\n",
    "all_data = all_data.join(areatype_data, on=['tmc'], how='left_outer')\n",
    "all_data = all_data.join(TMCattribute_data.withColumnRenamed(\"tmc_code\",\"tmc\"), on=['tmc','year'], how='left_outer')\n",
    "all_data = all_data.join(county_data.withColumnRenamed(\"tmc_code\",\"tmc\"), on=['tmc'], how='left_outer')\n",
    "all_data = all_data.join(dir_AADT_data.withColumnRenamed(\"Travel_Time_Code\",\"tmc\").withColumnRenamed(\"Year_Record\",\"year\"), on=['tmc','year'], how='left_outer')\n",
    "all_data = all_data.join(num_days_data.withColumnRenamed(\"Year\",\"year\"), on=['year'], how='left_outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace whitespace in column names\n",
    "all_data = all_data.select([F.col(col).alias(col.replace(' ', '_')) for col in all_data.columns])\n",
    "print(list(all_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add occupancy factor and observed LOTTR helper numerator and denominator terms\n",
    "\n",
    "# default occupancy factor\n",
    "occ_fac = 1.7\n",
    "\n",
    "all_data = all_data.withColumn(\"occ_fac\", F.lit(occ_fac))\n",
    "all_data = all_data.withColumn(\"obs_isReliable\", F.when(all_data['obs_rel_unrel'] == \"Rel\", 1).otherwise(0))\n",
    "all_data = all_data.withColumn(\"obs_LOTTR_helper_denominator\", all_data['miles_on_NHS'] * all_data['DIR_AADT'] * all_data['Number_Days'])\n",
    "all_data = all_data.withColumn(\"obs_LOTTR_helper_numerator\", all_data['obs_LOTTR_helper_denominator'] * all_data['obs_isReliable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add categorical fields for Terrain, and area type\n",
    "all_data = all_data.withColumn(\"Rolling\", F.when(all_data['Terrain'] == \"Rolling\", 1).otherwise(0))\n",
    "all_data = all_data.withColumn(\"Urbanized\", F.when(all_data['Area_Type'] == \"Urbanized\", 1).otherwise(0))\n",
    "all_data = all_data.withColumn(\"UrbanZCluster\", F.when(all_data['Area_Type'] == \"Urban Cluster\", 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop some columns that are not going to be used\n",
    "all_data = all_data.drop('road','dir','road_dir','Intersection','county','road order', 'global_road_order', 'Urban_Code','DIR_ADDT','District')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.toPandas().to_csv(\"all_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reformat Data\n",
    "\n",
    "The relatively small amount of rows could be an issue with training a classification algorithm. We are going to transform the data from wide to long format by separating all of the period columns into their own distinct dataframes, changing the column names to a universal name, and concatenating the dataframes. We'll then merge on all non-period associated columns. We'll save a copy of the response variable, OBS.rel_unrel, so that after running models we can do a grouby on year, tmc, and period, and compare to the original response format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate data\n",
    "non_period_cols = ['year', 'tmc', 'obs_rel_unrel', 'lanes', 'miles', 'Rolling',  'Urbanized', 'UrbanZCluster']\n",
    "AMP_cols = ['year','tmc', 'AMP_EPDOR', 'AMP_LIIR', 'AMP_SSP', 'AMP_Hour_TMS_Vol', 'AMP_TMS_Vol', 'AMP_Lane_TMS_Vol', 'AMP_Hour_Lane_TMS_Vol', 'AMP_Hour_NPMRDS_Vol', 'AMP_NPMRDS_Vol', 'AMP_Hour_Lane_NPMRDS_Vol', 'AMP_NPMRDS_straight_VoverC', 'AMP_TMS_straight_VoverC', 'AMP_NPMRDS_weighted_VoverC', 'AMP_TMS_weighted_VoverC', 'AMP_Truck']\n",
    "MIDD_cols = ['year','tmc', 'MIDD_EPDOR', 'MIDD_LIIR', 'MIDD_SSP', 'MIDD_Hour_TMS_Vol', 'MIDD_TMS_Vol', 'MIDD_Lane_TMS_Vol', 'MIDD_Hour_Lane_TMS_Vol', 'MIDD_Hour_NPMRDS_Vol', 'MIDD_NPMRDS_Vol', 'MIDD_Hour_Lane_NPMRDS_Vol', 'MIDD_NPMRDS_straight_VoverC', 'MIDD_TMS_straight_VoverC', 'MIDD_NPMRDS_weighted_VoverC', 'MIDD_TMS_weighted_VoverC', 'MIDD_Truck']\n",
    "PMP_cols = ['year','tmc', 'PMP_EPDOR', 'PMP_LIIR', 'PMP_SSP', 'PMP_Hour_TMS_Vol', 'PMP_TMS_Vol', 'PMP_Lane_TMS_Vol', 'PMP_Hour_Lane_TMS_Vol', 'PMP_Hour_NPMRDS_Vol', 'PMP_NPMRDS_Vol', 'PMP_Hour_Lane_NPMRDS_Vol', 'PMP_NPMRDS_straight_VoverC', 'PMP_TMS_straight_VoverC', 'PMP_NPMRDS_weighted_VoverC', 'PMP_TMS_weighted_VoverC', 'PMP_Truck']\n",
    "WE_cols = ['year','tmc', 'WE_EPDOR', 'WE_LIIR', 'WE_SSP', 'WE_Hour_TMS_Vol', 'WE_TMS_Vol', 'WE_Lane_TMS_Vol', 'WE_Hour_Lane_TMS_Vol', 'WE_Hour_NPMRDS_Vol', 'WE_NPMRDS_Vol', 'WE_Hour_Lane_NPMRDS_Vol', 'WE_NPMRDS_straight_VoverC', 'WE_TMS_straight_VoverC', 'WE_NPMRDS_weighted_VoverC', 'WE_TMS_weighted_VoverC', 'WE_Truck']\n",
    "FINAL_TEST_COLS = ['year', 'tmc', 'obs_rel_unrel']\n",
    "\n",
    "non_period_data = all_data[non_period_cols]\n",
    "AMP_data = all_data[AMP_cols]\n",
    "MIDD_data = all_data[MIDD_cols]\n",
    "PMP_data = all_data[PMP_cols]\n",
    "WE_data = all_data[WE_cols]\n",
    "FINAL_TEST_DATA = all_data[FINAL_TEST_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "def renameCols(data, name):\n",
    "    for col in data.columns:\n",
    "        data = data.withColumnRenamed(col,col.replace(name,\"\"))\n",
    "    return data\n",
    "\n",
    "AMP_data = renameCols(AMP_data,\"AMP_\")\n",
    "MIDD_data = renameCols(MIDD_data,\"MIDD_\")\n",
    "PMP_data = renameCols(PMP_data,\"PMP_\")\n",
    "WE_data = renameCols(WE_data,\"WE_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add period column\n",
    "AMP_data = AMP_data.withColumn(\"Period\", F.lit(\"AMP\"))\n",
    "MIDD_data = MIDD_data.withColumn(\"Period\", F.lit(\"MIDD\"))\n",
    "PMP_data = PMP_data.withColumn(\"Period\", F.lit(\"PMP\"))\n",
    "WE_data = WE_data.withColumn(\"Period\", F.lit(\"WE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate period data\n",
    "period_data = functools.reduce(lambda df1, df2: df1.union(df2), [AMP_data, MIDD_data, PMP_data, WE_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge wit the non-period columns\n",
    "data_final = period_data.join(non_period_data, on=['year','tmc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tempdf = data_final.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf.to_csv(\"data_long.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
